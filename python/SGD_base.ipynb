{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import load_data, preprocess_data, calculate_mse, build_index_groups\n",
    "import csv\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Note that `ratings` is a sparse matrix that in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "path_dataset = \"../data/data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncs45l1n/HyRRptq0/AuwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16c8a804240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data and return train and test data. TODO\n",
    "    # NOTE: we only consider users and movies that have more\n",
    "    # than 10 ratings\n",
    "    # ***************************************************\n",
    "    indices = valid_ratings.nonzero()\n",
    "    \n",
    "    train = sp.lil_matrix(valid_ratings.shape)\n",
    "    test = sp.lil_matrix(valid_ratings.shape)\n",
    "    for i, j in zip(indices[0], indices[1]):\n",
    "        r = np.random.random()\n",
    "        if r < 0.1:\n",
    "            test[i,j] = valid_ratings[i,j]\n",
    "        else:\n",
    "            train[i,j] = valid_ratings[i,j]\n",
    "    \n",
    "    print(\"Total number of nonzero elements in original data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nonzero elements in original data:1176952\n",
      "Total number of nonzero elements in train data:1058916\n",
      "Total number of nonzero elements in test data:117957\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASIAAAEbCAYAAAB+y4nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnIPIzIpzEJlZcf8PkkKFLzOuQCMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16c8caf8b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plots import plot_train_test_data\n",
    "\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "plot_train_test_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    model = NMF(n_components=num_features)\n",
    "    W = model.fit_transform(train, num_features)\n",
    "    Z = model.components_\n",
    "\n",
    "    return Z.T, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    \n",
    "    prediction = item_features.dot(user_features.T)\n",
    "    pred_new = []\n",
    "    real_data = []\n",
    "    for i, j in nz:\n",
    "        pred_new.append(prediction[i,j])\n",
    "        real_data.append(data[i,j])\n",
    "    \n",
    "    mse = calculate_mse(np.array(real_data), np.array(pred_new))/data.nnz\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD(train, test, gamma, k, lambda_u, lambda_i):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma_init = gamma\n",
    "    num_features = k # K in the lecture notes\n",
    "    lambda_user = lambda_u\n",
    "    lambda_item = lambda_i\n",
    "    num_epochs = 30     # number of full passes through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        counter = 0\n",
    "        prediction = item_features.dot(user_features.T)\n",
    "        for d, n in nz_train:\n",
    "\n",
    "            g_w = (train[d, n] - item_features[d].dot(user_features[n].T))*(user_features[n,:])\n",
    "            g_w = - (g_w - (lambda_item * item_features[d,:]))\n",
    "            g_z = (train[d, n].T - item_features[d].dot(user_features[n].T).T)*(item_features[d,:])\n",
    "            g_z = - (g_z - (lambda_user * user_features[n,:]))\n",
    "            \n",
    "            g_w = np.squeeze(np.asarray(g_w))\n",
    "            g_z = np.squeeze(np.asarray(g_z))\n",
    "            \n",
    "            item_features[d,:] -= gamma*g_w\n",
    "            user_features[n,:] -= gamma*g_z\n",
    "            \n",
    "        \n",
    "        rmse = compute_error(train, user_features, item_features, nz_train)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        errors.append(rmse)\n",
    "\n",
    "    rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data: {} with gamma={}, k={}, lambda_u={}, lambda_i={}.\".format(rmse, gamma_init, k, lambda_u, lambda_i))\n",
    "    return item_features, user_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_csv_submission(prediction):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: \n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    with open(\"../data/sampleSubmission.csv\", 'r') as sample:\n",
    "        data = sample.read().splitlines()[1:]\n",
    "    indices = [ re.match(r'r(\\d+?)_c(\\d+?),.*?', line, re.DOTALL).groups() for line in data ]\n",
    "        \n",
    "    with open(\"../data/submission.csv\", 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row, col in indices:\n",
    "            writer.writerow({'Id':\"r\" + row + \"_c\" + col,'Prediction':prediction[int(row) - 1, int(col) - 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.1024677421937312.\n",
      "iter: 1, RMSE on training set: 1.0482263228390165.\n",
      "iter: 2, RMSE on training set: 1.019158317208157.\n",
      "iter: 3, RMSE on training set: 0.9953355332030086.\n",
      "iter: 4, RMSE on training set: 0.9831839309120798.\n",
      "iter: 5, RMSE on training set: 0.9668419063169035.\n",
      "iter: 6, RMSE on training set: 0.9588065938383348.\n",
      "iter: 7, RMSE on training set: 0.9500004026200135.\n",
      "iter: 8, RMSE on training set: 0.9437638569979483.\n",
      "iter: 9, RMSE on training set: 0.9391468458521958.\n",
      "iter: 10, RMSE on training set: 0.9357825435140736.\n",
      "iter: 11, RMSE on training set: 0.9329066602611324.\n",
      "iter: 12, RMSE on training set: 0.9302530601908529.\n",
      "iter: 13, RMSE on training set: 0.9280879618249609.\n",
      "iter: 14, RMSE on training set: 0.9269764861861882.\n",
      "iter: 15, RMSE on training set: 0.9260773198664398.\n",
      "iter: 16, RMSE on training set: 0.924997613795191.\n",
      "iter: 17, RMSE on training set: 0.9243081318430134.\n",
      "iter: 18, RMSE on training set: 0.9238309734741224.\n",
      "iter: 19, RMSE on training set: 0.9234314136666286.\n",
      "iter: 20, RMSE on training set: 0.9232172656310061.\n",
      "iter: 21, RMSE on training set: 0.9228936811352008.\n",
      "iter: 22, RMSE on training set: 0.9226790294618102.\n",
      "iter: 23, RMSE on training set: 0.9225245280391382.\n",
      "iter: 24, RMSE on training set: 0.9224348945058136.\n",
      "iter: 25, RMSE on training set: 0.9223573373764905.\n",
      "iter: 26, RMSE on training set: 0.9223064460252429.\n",
      "iter: 27, RMSE on training set: 0.9222627127527231.\n",
      "iter: 28, RMSE on training set: 0.9222507663443591.\n",
      "iter: 29, RMSE on training set: 0.9222205228197016.\n",
      "RMSE on test data: 0.9757663011192595 with gamma=0.04, k=3, lambda_u=0.01, lambda_i=0.01.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.1148994398543697.\n",
      "iter: 1, RMSE on training set: 1.0607755893349233.\n",
      "iter: 2, RMSE on training set: 1.0304266860760811.\n",
      "iter: 3, RMSE on training set: 1.0056172475688578.\n",
      "iter: 4, RMSE on training set: 0.9912897415910108.\n",
      "iter: 5, RMSE on training set: 0.9744964279034048.\n",
      "iter: 6, RMSE on training set: 0.9656051920500688.\n",
      "iter: 7, RMSE on training set: 0.9549768676265188.\n",
      "iter: 8, RMSE on training set: 0.948723463254038.\n",
      "iter: 9, RMSE on training set: 0.9426274110312266.\n",
      "iter: 10, RMSE on training set: 0.9381891695192254.\n",
      "iter: 11, RMSE on training set: 0.9347432635141768.\n",
      "iter: 12, RMSE on training set: 0.9314438852449738.\n",
      "iter: 13, RMSE on training set: 0.9288890021321693.\n",
      "iter: 14, RMSE on training set: 0.9273190400996398.\n",
      "iter: 15, RMSE on training set: 0.9259604715153015.\n",
      "iter: 16, RMSE on training set: 0.924700449543638.\n",
      "iter: 17, RMSE on training set: 0.9240102998532521.\n",
      "iter: 18, RMSE on training set: 0.9233768771163376.\n",
      "iter: 19, RMSE on training set: 0.9230043885087265.\n",
      "iter: 20, RMSE on training set: 0.9228324018325681.\n",
      "iter: 21, RMSE on training set: 0.9225327591921222.\n",
      "iter: 22, RMSE on training set: 0.9223792025258651.\n",
      "iter: 23, RMSE on training set: 0.9222724145052651.\n",
      "iter: 24, RMSE on training set: 0.9221923051462956.\n",
      "iter: 25, RMSE on training set: 0.9221733225027628.\n",
      "iter: 26, RMSE on training set: 0.9221453592739073.\n",
      "iter: 27, RMSE on training set: 0.9221164096726691.\n",
      "iter: 28, RMSE on training set: 0.9221158224131184.\n",
      "iter: 29, RMSE on training set: 0.9220904021725521.\n",
      "RMSE on test data: 0.9741892585021766 with gamma=0.04, k=3, lambda_u=0.01, lambda_i=0.05.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.1330085176568063.\n",
      "iter: 1, RMSE on training set: 1.0808879671176763.\n",
      "iter: 2, RMSE on training set: 1.0502007103363025.\n",
      "iter: 3, RMSE on training set: 1.0237281078713196.\n",
      "iter: 4, RMSE on training set: 1.0070787854443861.\n",
      "iter: 5, RMSE on training set: 0.9886957046835748.\n",
      "iter: 6, RMSE on training set: 0.9786194046957724.\n",
      "iter: 7, RMSE on training set: 0.9654906861685212.\n",
      "iter: 8, RMSE on training set: 0.9583337657045804.\n",
      "iter: 9, RMSE on training set: 0.9506602736032046.\n",
      "iter: 10, RMSE on training set: 0.9448866826007957.\n",
      "iter: 11, RMSE on training set: 0.9406339088259338.\n",
      "iter: 12, RMSE on training set: 0.9362787230018476.\n",
      "iter: 13, RMSE on training set: 0.9330417854152755.\n",
      "iter: 14, RMSE on training set: 0.9307233836317979.\n",
      "iter: 15, RMSE on training set: 0.9287690406341575.\n",
      "iter: 16, RMSE on training set: 0.9269957847221958.\n",
      "iter: 17, RMSE on training set: 0.9260768473125754.\n",
      "iter: 18, RMSE on training set: 0.9251046950405449.\n",
      "iter: 19, RMSE on training set: 0.9245571022296953.\n",
      "iter: 20, RMSE on training set: 0.924294510341509.\n",
      "iter: 21, RMSE on training set: 0.9238930246977357.\n",
      "iter: 22, RMSE on training set: 0.9236922427585771.\n",
      "iter: 23, RMSE on training set: 0.9235482942958585.\n",
      "iter: 24, RMSE on training set: 0.9234496646911607.\n",
      "iter: 25, RMSE on training set: 0.9234346046361835.\n",
      "iter: 26, RMSE on training set: 0.9234008882146046.\n",
      "iter: 27, RMSE on training set: 0.9233654779623209.\n",
      "iter: 28, RMSE on training set: 0.9233657828300502.\n",
      "iter: 29, RMSE on training set: 0.9233376392486223.\n",
      "RMSE on test data: 0.9734124355767971 with gamma=0.04, k=3, lambda_u=0.01, lambda_i=0.1.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0882110712228183.\n",
      "iter: 1, RMSE on training set: 1.04677654996138.\n",
      "iter: 2, RMSE on training set: 1.0272411824112169.\n",
      "iter: 3, RMSE on training set: 1.004949824916696.\n",
      "iter: 4, RMSE on training set: 0.9971526042670485.\n",
      "iter: 5, RMSE on training set: 0.980300451099909.\n",
      "iter: 6, RMSE on training set: 0.9730150530874562.\n",
      "iter: 7, RMSE on training set: 0.9627559574624469.\n",
      "iter: 8, RMSE on training set: 0.9552231055295227.\n",
      "iter: 9, RMSE on training set: 0.9512316483596569.\n",
      "iter: 10, RMSE on training set: 0.948187817159134.\n",
      "iter: 11, RMSE on training set: 0.9450811556933845.\n",
      "iter: 12, RMSE on training set: 0.942010202022497.\n",
      "iter: 13, RMSE on training set: 0.9391829302453344.\n",
      "iter: 14, RMSE on training set: 0.9379724623678178.\n",
      "iter: 15, RMSE on training set: 0.9370207502570986.\n",
      "iter: 16, RMSE on training set: 0.9351226510108223.\n",
      "iter: 17, RMSE on training set: 0.9343694483733427.\n",
      "iter: 18, RMSE on training set: 0.9334874744681296.\n",
      "iter: 19, RMSE on training set: 0.9328760388550684.\n",
      "iter: 20, RMSE on training set: 0.9324687356008896.\n",
      "iter: 21, RMSE on training set: 0.9318646420246686.\n",
      "iter: 22, RMSE on training set: 0.9313837151541328.\n",
      "iter: 23, RMSE on training set: 0.9310511601677743.\n",
      "iter: 24, RMSE on training set: 0.9308170159731278.\n",
      "iter: 25, RMSE on training set: 0.9306698992244763.\n",
      "iter: 26, RMSE on training set: 0.9305670864511522.\n",
      "iter: 27, RMSE on training set: 0.9304427452237601.\n",
      "iter: 28, RMSE on training set: 0.9304325721600384.\n",
      "iter: 29, RMSE on training set: 0.9303120825282042.\n",
      "RMSE on test data: 0.9758806932905092 with gamma=0.04, k=3, lambda_u=0.05, lambda_i=0.01.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0930511929729632.\n",
      "iter: 1, RMSE on training set: 1.0495527512971603.\n",
      "iter: 2, RMSE on training set: 1.0275689239078787.\n",
      "iter: 3, RMSE on training set: 1.0041043849805076.\n",
      "iter: 4, RMSE on training set: 0.9944921846458162.\n",
      "iter: 5, RMSE on training set: 0.9780634808065332.\n",
      "iter: 6, RMSE on training set: 0.9707808443520606.\n",
      "iter: 7, RMSE on training set: 0.9605248684974316.\n",
      "iter: 8, RMSE on training set: 0.9545194930009472.\n",
      "iter: 9, RMSE on training set: 0.9502666874861938.\n",
      "iter: 10, RMSE on training set: 0.9465347746824984.\n",
      "iter: 11, RMSE on training set: 0.9436775469160656.\n",
      "iter: 12, RMSE on training set: 0.941335251933845.\n",
      "iter: 13, RMSE on training set: 0.9388667978423245.\n",
      "iter: 14, RMSE on training set: 0.9378292939890613.\n",
      "iter: 15, RMSE on training set: 0.9369193300582874.\n",
      "iter: 16, RMSE on training set: 0.9353759369281117.\n",
      "iter: 17, RMSE on training set: 0.934878945814352.\n",
      "iter: 18, RMSE on training set: 0.9341290778678358.\n",
      "iter: 19, RMSE on training set: 0.9337781873915807.\n",
      "iter: 20, RMSE on training set: 0.9336492132365942.\n",
      "iter: 21, RMSE on training set: 0.9331417016260781.\n",
      "iter: 22, RMSE on training set: 0.9328173437275217.\n",
      "iter: 23, RMSE on training set: 0.9326563200625579.\n",
      "iter: 24, RMSE on training set: 0.9324463412841754.\n",
      "iter: 25, RMSE on training set: 0.9324466025046521.\n",
      "iter: 26, RMSE on training set: 0.9324081835922547.\n",
      "iter: 27, RMSE on training set: 0.9323358697673553.\n",
      "iter: 28, RMSE on training set: 0.932361836384288.\n",
      "iter: 29, RMSE on training set: 0.9322615997599013.\n",
      "RMSE on test data: 0.9749607200827716 with gamma=0.04, k=3, lambda_u=0.05, lambda_i=0.05.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.1025466917026305.\n",
      "iter: 1, RMSE on training set: 1.0597569444837938.\n",
      "iter: 2, RMSE on training set: 1.037074595821077.\n",
      "iter: 3, RMSE on training set: 1.012986719652579.\n",
      "iter: 4, RMSE on training set: 1.0021597947553198.\n",
      "iter: 5, RMSE on training set: 0.9858023489904186.\n",
      "iter: 6, RMSE on training set: 0.9782390402141045.\n",
      "iter: 7, RMSE on training set: 0.9671143365209394.\n",
      "iter: 8, RMSE on training set: 0.9613909805932416.\n",
      "iter: 9, RMSE on training set: 0.9565462615833468.\n",
      "iter: 10, RMSE on training set: 0.9521183718980925.\n",
      "iter: 11, RMSE on training set: 0.9491282844002846.\n",
      "iter: 12, RMSE on training set: 0.946757997758379.\n",
      "iter: 13, RMSE on training set: 0.9442473235987859.\n",
      "iter: 14, RMSE on training set: 0.9430529105065821.\n",
      "iter: 15, RMSE on training set: 0.9420042734240409.\n",
      "iter: 16, RMSE on training set: 0.9404380096348791.\n",
      "iter: 17, RMSE on training set: 0.9400632609006516.\n",
      "iter: 18, RMSE on training set: 0.939257528850825.\n",
      "iter: 19, RMSE on training set: 0.939002391894388.\n",
      "iter: 20, RMSE on training set: 0.9389996363718087.\n",
      "iter: 21, RMSE on training set: 0.9384902449525001.\n",
      "iter: 22, RMSE on training set: 0.9382442999129968.\n",
      "iter: 23, RMSE on training set: 0.9381340436556687.\n",
      "iter: 24, RMSE on training set: 0.9379397954680968.\n",
      "iter: 25, RMSE on training set: 0.93799177154364.\n",
      "iter: 26, RMSE on training set: 0.9379839028319674.\n",
      "iter: 27, RMSE on training set: 0.9379256021099682.\n",
      "iter: 28, RMSE on training set: 0.9379644640360225.\n",
      "iter: 29, RMSE on training set: 0.93787511562385.\n",
      "RMSE on test data: 0.9770370685793338 with gamma=0.04, k=3, lambda_u=0.05, lambda_i=0.1.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0961550935750461.\n",
      "iter: 1, RMSE on training set: 1.0604467987876807.\n",
      "iter: 2, RMSE on training set: 1.045108778633831.\n",
      "iter: 3, RMSE on training set: 1.023352197493145.\n",
      "iter: 4, RMSE on training set: 1.017019610240449.\n",
      "iter: 5, RMSE on training set: 0.9992110170801847.\n",
      "iter: 6, RMSE on training set: 0.9912625041468232.\n",
      "iter: 7, RMSE on training set: 0.9779395081170682.\n",
      "iter: 8, RMSE on training set: 0.9691689665044516.\n",
      "iter: 9, RMSE on training set: 0.9644076522835876.\n",
      "iter: 10, RMSE on training set: 0.9616340121431198.\n",
      "iter: 11, RMSE on training set: 0.9578864589645687.\n",
      "iter: 12, RMSE on training set: 0.9539802682948787.\n",
      "iter: 13, RMSE on training set: 0.9501033804846895.\n",
      "iter: 14, RMSE on training set: 0.9488405760686852.\n",
      "iter: 15, RMSE on training set: 0.9475785466470675.\n",
      "iter: 16, RMSE on training set: 0.9449627250643314.\n",
      "iter: 17, RMSE on training set: 0.9442137149150754.\n",
      "iter: 18, RMSE on training set: 0.9428336426474665.\n",
      "iter: 19, RMSE on training set: 0.9421128853787474.\n",
      "iter: 20, RMSE on training set: 0.9415278670231615.\n",
      "iter: 21, RMSE on training set: 0.9407139031406871.\n",
      "iter: 22, RMSE on training set: 0.9399796627470393.\n",
      "iter: 23, RMSE on training set: 0.9394845918006167.\n",
      "iter: 24, RMSE on training set: 0.9391047446446323.\n",
      "iter: 25, RMSE on training set: 0.9389322647821567.\n",
      "iter: 26, RMSE on training set: 0.9387929643983685.\n",
      "iter: 27, RMSE on training set: 0.9385842734050761.\n",
      "iter: 28, RMSE on training set: 0.9385964107720489.\n",
      "iter: 29, RMSE on training set: 0.9383743439539873.\n",
      "RMSE on test data: 0.9780213292349671 with gamma=0.04, k=3, lambda_u=0.1, lambda_i=0.01.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0969915671633623.\n",
      "iter: 1, RMSE on training set: 1.0585625178347224.\n",
      "iter: 2, RMSE on training set: 1.0414161002435336.\n",
      "iter: 3, RMSE on training set: 1.0181937621746409.\n",
      "iter: 4, RMSE on training set: 1.0108924273532507.\n",
      "iter: 5, RMSE on training set: 0.9940696258044499.\n",
      "iter: 6, RMSE on training set: 0.9872690216244179.\n",
      "iter: 7, RMSE on training set: 0.9751139778297485.\n",
      "iter: 8, RMSE on training set: 0.9686391681012639.\n",
      "iter: 9, RMSE on training set: 0.96465303047055.\n",
      "iter: 10, RMSE on training set: 0.9610820509779.\n",
      "iter: 11, RMSE on training set: 0.9580806253724191.\n",
      "iter: 12, RMSE on training set: 0.955622447977434.\n",
      "iter: 13, RMSE on training set: 0.9525360332365653.\n",
      "iter: 14, RMSE on training set: 0.951531529654491.\n",
      "iter: 15, RMSE on training set: 0.9506279423917992.\n",
      "iter: 16, RMSE on training set: 0.9485277433874383.\n",
      "iter: 17, RMSE on training set: 0.9480830974390598.\n",
      "iter: 18, RMSE on training set: 0.9470200095583966.\n",
      "iter: 19, RMSE on training set: 0.9466309228831714.\n",
      "iter: 20, RMSE on training set: 0.9464342658950843.\n",
      "iter: 21, RMSE on training set: 0.9457387995416305.\n",
      "iter: 22, RMSE on training set: 0.9451867433063739.\n",
      "iter: 23, RMSE on training set: 0.9449738076672667.\n",
      "iter: 24, RMSE on training set: 0.9446068282973091.\n",
      "iter: 25, RMSE on training set: 0.9446442745841833.\n",
      "iter: 26, RMSE on training set: 0.9445979159147392.\n",
      "iter: 27, RMSE on training set: 0.9444713621390837.\n",
      "iter: 28, RMSE on training set: 0.9445325505675602.\n",
      "iter: 29, RMSE on training set: 0.9443381662290311.\n",
      "RMSE on test data: 0.9800353937881054 with gamma=0.04, k=3, lambda_u=0.1, lambda_i=0.05.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.102743589063389.\n",
      "iter: 1, RMSE on training set: 1.0653939357755227.\n",
      "iter: 2, RMSE on training set: 1.0487555620317959.\n",
      "iter: 3, RMSE on training set: 1.0257615746301931.\n",
      "iter: 4, RMSE on training set: 1.0183173844058155.\n",
      "iter: 5, RMSE on training set: 1.0025910459369642.\n",
      "iter: 6, RMSE on training set: 0.9964059718299848.\n",
      "iter: 7, RMSE on training set: 0.9842217914352858.\n",
      "iter: 8, RMSE on training set: 0.978802497181531.\n",
      "iter: 9, RMSE on training set: 0.9749077286758266.\n",
      "iter: 10, RMSE on training set: 0.9706600251610026.\n",
      "iter: 11, RMSE on training set: 0.9678828479676026.\n",
      "iter: 12, RMSE on training set: 0.9659913101865599.\n",
      "iter: 13, RMSE on training set: 0.9631309725836834.\n",
      "iter: 14, RMSE on training set: 0.9621468792391201.\n",
      "iter: 15, RMSE on training set: 0.9612674009269494.\n",
      "iter: 16, RMSE on training set: 0.959286242948285.\n",
      "iter: 17, RMSE on training set: 0.9590832997237216.\n",
      "iter: 18, RMSE on training set: 0.9580335986816022.\n",
      "iter: 19, RMSE on training set: 0.9578246127665192.\n",
      "iter: 20, RMSE on training set: 0.9578447008864782.\n",
      "iter: 21, RMSE on training set: 0.957156239698985.\n",
      "iter: 22, RMSE on training set: 0.956709951052661.\n",
      "iter: 23, RMSE on training set: 0.9566045850748391.\n",
      "iter: 24, RMSE on training set: 0.9562542823596807.\n",
      "iter: 25, RMSE on training set: 0.9563708370966352.\n",
      "iter: 26, RMSE on training set: 0.9563794686799272.\n",
      "iter: 27, RMSE on training set: 0.9562799399356924.\n",
      "iter: 28, RMSE on training set: 0.9563655249557891.\n",
      "iter: 29, RMSE on training set: 0.9561879407501874.\n",
      "RMSE on test data: 0.987193070832048 with gamma=0.04, k=3, lambda_u=0.1, lambda_i=0.1.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0918477823813992.\n",
      "iter: 1, RMSE on training set: 1.0368776855188773.\n",
      "iter: 2, RMSE on training set: 1.005863973938572.\n",
      "iter: 3, RMSE on training set: 0.9817673453567728.\n",
      "iter: 4, RMSE on training set: 0.964026418624883.\n",
      "iter: 5, RMSE on training set: 0.9489461739037729.\n",
      "iter: 6, RMSE on training set: 0.9394969096168971.\n",
      "iter: 7, RMSE on training set: 0.9323894709760129.\n",
      "iter: 8, RMSE on training set: 0.9247955684104225.\n",
      "iter: 9, RMSE on training set: 0.919725887515264.\n",
      "iter: 10, RMSE on training set: 0.9165020088874073.\n",
      "iter: 11, RMSE on training set: 0.9123604626839724.\n",
      "iter: 12, RMSE on training set: 0.9101704413186574.\n",
      "iter: 13, RMSE on training set: 0.9084837834870042.\n",
      "iter: 14, RMSE on training set: 0.9067081601652761.\n",
      "iter: 15, RMSE on training set: 0.9056076760405674.\n",
      "iter: 16, RMSE on training set: 0.9051183314101336.\n",
      "iter: 17, RMSE on training set: 0.9042923374906342.\n",
      "iter: 18, RMSE on training set: 0.9037889001767652.\n",
      "iter: 19, RMSE on training set: 0.9035116526512423.\n",
      "iter: 20, RMSE on training set: 0.9031909927829952.\n",
      "iter: 21, RMSE on training set: 0.9028670982296574.\n",
      "iter: 22, RMSE on training set: 0.9026736413347646.\n",
      "iter: 23, RMSE on training set: 0.9025631631764096.\n",
      "iter: 24, RMSE on training set: 0.9024420962855171.\n",
      "iter: 25, RMSE on training set: 0.9024013725556417.\n",
      "iter: 26, RMSE on training set: 0.9023374677500964.\n",
      "iter: 27, RMSE on training set: 0.9022904548559285.\n",
      "iter: 28, RMSE on training set: 0.9022532523372494.\n",
      "iter: 29, RMSE on training set: 0.9022323832608722.\n",
      "RMSE on test data: 0.9762102246301796 with gamma=0.04, k=4, lambda_u=0.01, lambda_i=0.01.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.1042865416652166.\n",
      "iter: 1, RMSE on training set: 1.0482981728731775.\n",
      "iter: 2, RMSE on training set: 1.0161857904932479.\n",
      "iter: 3, RMSE on training set: 0.9905194254526025.\n",
      "iter: 4, RMSE on training set: 0.9723578967187836.\n",
      "iter: 5, RMSE on training set: 0.9567231750052985.\n",
      "iter: 6, RMSE on training set: 0.946219218810487.\n",
      "iter: 7, RMSE on training set: 0.9377567155955092.\n",
      "iter: 8, RMSE on training set: 0.929126672683674.\n",
      "iter: 9, RMSE on training set: 0.9232088069083219.\n",
      "iter: 10, RMSE on training set: 0.9190488376040761.\n",
      "iter: 11, RMSE on training set: 0.9143969938664773.\n",
      "iter: 12, RMSE on training set: 0.9116694850131732.\n",
      "iter: 13, RMSE on training set: 0.9092930781665239.\n",
      "iter: 14, RMSE on training set: 0.907135142044701.\n",
      "iter: 15, RMSE on training set: 0.9058788756759655.\n",
      "iter: 16, RMSE on training set: 0.9051261704323168.\n",
      "iter: 17, RMSE on training set: 0.9041339533361297.\n",
      "iter: 18, RMSE on training set: 0.9036186984506333.\n",
      "iter: 19, RMSE on training set: 0.9032885919506427.\n",
      "iter: 20, RMSE on training set: 0.9030127554869727.\n",
      "iter: 21, RMSE on training set: 0.9027094676519343.\n",
      "iter: 22, RMSE on training set: 0.9025618697050769.\n",
      "iter: 23, RMSE on training set: 0.9024937185029885.\n",
      "iter: 24, RMSE on training set: 0.9024128687800628.\n",
      "iter: 25, RMSE on training set: 0.9023797864578154.\n",
      "iter: 26, RMSE on training set: 0.9023359728304369.\n",
      "iter: 27, RMSE on training set: 0.9023067608865369.\n",
      "iter: 28, RMSE on training set: 0.9022834071923466.\n",
      "iter: 29, RMSE on training set: 0.9022686504108548.\n",
      "RMSE on test data: 0.9736036324821207 with gamma=0.04, k=4, lambda_u=0.01, lambda_i=0.05.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.1226660764104803.\n",
      "iter: 1, RMSE on training set: 1.0684068589506632.\n",
      "iter: 2, RMSE on training set: 1.0353256058804403.\n",
      "iter: 3, RMSE on training set: 1.0085068397110972.\n",
      "iter: 4, RMSE on training set: 0.9892856220975443.\n",
      "iter: 5, RMSE on training set: 0.9717435679816159.\n",
      "iter: 6, RMSE on training set: 0.9593618671796048.\n",
      "iter: 7, RMSE on training set: 0.9494987970107747.\n",
      "iter: 8, RMSE on training set: 0.9388345398054659.\n",
      "iter: 9, RMSE on training set: 0.9318119950398149.\n",
      "iter: 10, RMSE on training set: 0.9262978212699773.\n",
      "iter: 11, RMSE on training set: 0.9206440249713123.\n",
      "iter: 12, RMSE on training set: 0.9171725071262601.\n",
      "iter: 13, RMSE on training set: 0.9139052500465182.\n",
      "iter: 14, RMSE on training set: 0.9110195086973629.\n",
      "iter: 15, RMSE on training set: 0.9092394646334845.\n",
      "iter: 16, RMSE on training set: 0.9080707541717548.\n",
      "iter: 17, RMSE on training set: 0.9066873250890196.\n",
      "iter: 18, RMSE on training set: 0.9059226938897134.\n",
      "iter: 19, RMSE on training set: 0.9053975883430491.\n",
      "iter: 20, RMSE on training set: 0.9050199067220237.\n",
      "iter: 21, RMSE on training set: 0.9046055253094338.\n",
      "iter: 22, RMSE on training set: 0.9044090680740435.\n",
      "iter: 23, RMSE on training set: 0.9043197705363212.\n",
      "iter: 24, RMSE on training set: 0.9042228616544994.\n",
      "iter: 25, RMSE on training set: 0.9041690547396941.\n",
      "iter: 26, RMSE on training set: 0.904119937318818.\n",
      "iter: 27, RMSE on training set: 0.904085916000994.\n",
      "iter: 28, RMSE on training set: 0.9040615638676095.\n",
      "iter: 29, RMSE on training set: 0.9040441423561619.\n",
      "RMSE on test data: 0.9717782765812946 with gamma=0.04, k=4, lambda_u=0.01, lambda_i=0.1.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0815228824910212.\n",
      "iter: 1, RMSE on training set: 1.0457532067110693.\n",
      "iter: 2, RMSE on training set: 1.0177002318720823.\n",
      "iter: 3, RMSE on training set: 1.000254093946876.\n",
      "iter: 4, RMSE on training set: 0.9804201120077849.\n",
      "iter: 5, RMSE on training set: 0.9650625012635263.\n",
      "iter: 6, RMSE on training set: 0.9543616455271957.\n",
      "iter: 7, RMSE on training set: 0.9491533083288363.\n",
      "iter: 8, RMSE on training set: 0.939601679090424.\n",
      "iter: 9, RMSE on training set: 0.935025622374096.\n",
      "iter: 10, RMSE on training set: 0.9312609743210813.\n",
      "iter: 11, RMSE on training set: 0.9261471919510744.\n",
      "iter: 12, RMSE on training set: 0.9241066746940677.\n",
      "iter: 13, RMSE on training set: 0.9217344847159784.\n",
      "iter: 14, RMSE on training set: 0.9189287137857521.\n",
      "iter: 15, RMSE on training set: 0.9175185898881486.\n",
      "iter: 16, RMSE on training set: 0.9174029445585795.\n",
      "iter: 17, RMSE on training set: 0.9159132746018492.\n",
      "iter: 18, RMSE on training set: 0.9149778392441197.\n",
      "iter: 19, RMSE on training set: 0.9145882156977124.\n",
      "iter: 20, RMSE on training set: 0.9140717153717237.\n",
      "iter: 21, RMSE on training set: 0.9133973301445037.\n",
      "iter: 22, RMSE on training set: 0.9129476132231915.\n",
      "iter: 23, RMSE on training set: 0.9127943157274776.\n",
      "iter: 24, RMSE on training set: 0.912472594801357.\n",
      "iter: 25, RMSE on training set: 0.9124741423756847.\n",
      "iter: 26, RMSE on training set: 0.9122819160069995.\n",
      "iter: 27, RMSE on training set: 0.9121026869129497.\n",
      "iter: 28, RMSE on training set: 0.9120493578746661.\n",
      "iter: 29, RMSE on training set: 0.9119731398990423.\n",
      "RMSE on test data: 0.9729787833467547 with gamma=0.04, k=4, lambda_u=0.05, lambda_i=0.01.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.086680701409866.\n",
      "iter: 1, RMSE on training set: 1.0469931272714355.\n",
      "iter: 2, RMSE on training set: 1.016912766978644.\n",
      "iter: 3, RMSE on training set: 0.9976061130150347.\n",
      "iter: 4, RMSE on training set: 0.9786504563170197.\n",
      "iter: 5, RMSE on training set: 0.9636959719260744.\n",
      "iter: 6, RMSE on training set: 0.9535669113975632.\n",
      "iter: 7, RMSE on training set: 0.9472845686816788.\n",
      "iter: 8, RMSE on training set: 0.9389221458427639.\n",
      "iter: 9, RMSE on training set: 0.9341899315833297.\n",
      "iter: 10, RMSE on training set: 0.9307802783328799.\n",
      "iter: 11, RMSE on training set: 0.9260347264545223.\n",
      "iter: 12, RMSE on training set: 0.9243943287690989.\n",
      "iter: 13, RMSE on training set: 0.9221630265307772.\n",
      "iter: 14, RMSE on training set: 0.9197131203919126.\n",
      "iter: 15, RMSE on training set: 0.9188419208612841.\n",
      "iter: 16, RMSE on training set: 0.9189071190719933.\n",
      "iter: 17, RMSE on training set: 0.9175599588769785.\n",
      "iter: 18, RMSE on training set: 0.9170436516170175.\n",
      "iter: 19, RMSE on training set: 0.9168357746925513.\n",
      "iter: 20, RMSE on training set: 0.9164494592841612.\n",
      "iter: 21, RMSE on training set: 0.9158836414044128.\n",
      "iter: 22, RMSE on training set: 0.9156318814924668.\n",
      "iter: 23, RMSE on training set: 0.9155839448130352.\n",
      "iter: 24, RMSE on training set: 0.9153777351080798.\n",
      "iter: 25, RMSE on training set: 0.9154165425659175.\n",
      "iter: 26, RMSE on training set: 0.9152705097508039.\n",
      "iter: 27, RMSE on training set: 0.9151705000432715.\n",
      "iter: 28, RMSE on training set: 0.9151359104203317.\n",
      "iter: 29, RMSE on training set: 0.9150966087825043.\n",
      "RMSE on test data: 0.9710591124823931 with gamma=0.04, k=4, lambda_u=0.05, lambda_i=0.05.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0968579227896837.\n",
      "iter: 1, RMSE on training set: 1.0567526397839055.\n",
      "iter: 2, RMSE on training set: 1.026569118688064.\n",
      "iter: 3, RMSE on training set: 1.0067365461928561.\n",
      "iter: 4, RMSE on training set: 0.988295385537526.\n",
      "iter: 5, RMSE on training set: 0.9735008946782804.\n",
      "iter: 6, RMSE on training set: 0.9628984851740501.\n",
      "iter: 7, RMSE on training set: 0.9559099629827434.\n",
      "iter: 8, RMSE on training set: 0.9475184439417745.\n",
      "iter: 9, RMSE on training set: 0.9422710143146927.\n",
      "iter: 10, RMSE on training set: 0.938537330989737.\n",
      "iter: 11, RMSE on training set: 0.9336334994193579.\n",
      "iter: 12, RMSE on training set: 0.9319514864457112.\n",
      "iter: 13, RMSE on training set: 0.9293981463387297.\n",
      "iter: 14, RMSE on training set: 0.9268064632027313.\n",
      "iter: 15, RMSE on training set: 0.9261161703098664.\n",
      "iter: 16, RMSE on training set: 0.9261714964613611.\n",
      "iter: 17, RMSE on training set: 0.9247707747225683.\n",
      "iter: 18, RMSE on training set: 0.924394026865127.\n",
      "iter: 19, RMSE on training set: 0.924195114688566.\n",
      "iter: 20, RMSE on training set: 0.9238751135174544.\n",
      "iter: 21, RMSE on training set: 0.9233538093343312.\n",
      "iter: 22, RMSE on training set: 0.9231510092107935.\n",
      "iter: 23, RMSE on training set: 0.9231474989739452.\n",
      "iter: 24, RMSE on training set: 0.9229871007855408.\n",
      "iter: 25, RMSE on training set: 0.9230306935287536.\n",
      "iter: 26, RMSE on training set: 0.9229038809700288.\n",
      "iter: 27, RMSE on training set: 0.9228294686480982.\n",
      "iter: 28, RMSE on training set: 0.922798474330862.\n",
      "iter: 29, RMSE on training set: 0.9227699221683348.\n",
      "RMSE on test data: 0.9726846892760995 with gamma=0.04, k=4, lambda_u=0.05, lambda_i=0.1.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0930563667446478.\n",
      "iter: 1, RMSE on training set: 1.0685456683826875.\n",
      "iter: 2, RMSE on training set: 1.040131297891098.\n",
      "iter: 3, RMSE on training set: 1.0267507410537542.\n",
      "iter: 4, RMSE on training set: 1.0027949016263418.\n",
      "iter: 5, RMSE on training set: 0.9872745202080236.\n",
      "iter: 6, RMSE on training set: 0.9733861740032979.\n",
      "iter: 7, RMSE on training set: 0.9690776349486903.\n",
      "iter: 8, RMSE on training set: 0.9569005590825728.\n",
      "iter: 9, RMSE on training set: 0.9519123000865667.\n",
      "iter: 10, RMSE on training set: 0.94721277453872.\n",
      "iter: 11, RMSE on training set: 0.9409887175378062.\n",
      "iter: 12, RMSE on training set: 0.9386899873424477.\n",
      "iter: 13, RMSE on training set: 0.9353897656574335.\n",
      "iter: 14, RMSE on training set: 0.9313567670044357.\n",
      "iter: 15, RMSE on training set: 0.9296867369512422.\n",
      "iter: 16, RMSE on training set: 0.9298648741335201.\n",
      "iter: 17, RMSE on training set: 0.927694591474365.\n",
      "iter: 18, RMSE on training set: 0.9262744628973023.\n",
      "iter: 19, RMSE on training set: 0.9257239214171941.\n",
      "iter: 20, RMSE on training set: 0.9250856030067361.\n",
      "iter: 21, RMSE on training set: 0.9241428380204852.\n",
      "iter: 22, RMSE on training set: 0.9234584173285152.\n",
      "iter: 23, RMSE on training set: 0.9232601629109075.\n",
      "iter: 24, RMSE on training set: 0.9227604412446099.\n",
      "iter: 25, RMSE on training set: 0.9228423130420303.\n",
      "iter: 26, RMSE on training set: 0.9225345807273502.\n",
      "iter: 27, RMSE on training set: 0.9221766727394258.\n",
      "iter: 28, RMSE on training set: 0.9221707653232656.\n",
      "iter: 29, RMSE on training set: 0.9220140875836137.\n",
      "RMSE on test data: 0.9736296457520385 with gamma=0.04, k=4, lambda_u=0.1, lambda_i=0.01.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.094185273410579.\n",
      "iter: 1, RMSE on training set: 1.065454071640349.\n",
      "iter: 2, RMSE on training set: 1.0351782840811379.\n",
      "iter: 3, RMSE on training set: 1.020273321877446.\n",
      "iter: 4, RMSE on training set: 0.9988841283245855.\n",
      "iter: 5, RMSE on training set: 0.9840391637440168.\n",
      "iter: 6, RMSE on training set: 0.9723255291665275.\n",
      "iter: 7, RMSE on training set: 0.9674256228934892.\n",
      "iter: 8, RMSE on training set: 0.9572745780993694.\n",
      "iter: 9, RMSE on training set: 0.9530312364944767.\n",
      "iter: 10, RMSE on training set: 0.9493564861178486.\n",
      "iter: 11, RMSE on training set: 0.9437876954639416.\n",
      "iter: 12, RMSE on training set: 0.9424344714521823.\n",
      "iter: 13, RMSE on training set: 0.9396523891103593.\n",
      "iter: 14, RMSE on training set: 0.9363421947004823.\n",
      "iter: 15, RMSE on training set: 0.9353844137247905.\n",
      "iter: 16, RMSE on training set: 0.9359962655089814.\n",
      "iter: 17, RMSE on training set: 0.9340591577068509.\n",
      "iter: 18, RMSE on training set: 0.9333156451740088.\n",
      "iter: 19, RMSE on training set: 0.9331029794984977.\n",
      "iter: 20, RMSE on training set: 0.9325818762310807.\n",
      "iter: 21, RMSE on training set: 0.9317279927655022.\n",
      "iter: 22, RMSE on training set: 0.9313632739471353.\n",
      "iter: 23, RMSE on training set: 0.9313211917560853.\n",
      "iter: 24, RMSE on training set: 0.9309652965369454.\n",
      "iter: 25, RMSE on training set: 0.9311196626782174.\n",
      "iter: 26, RMSE on training set: 0.9308528725990484.\n",
      "iter: 27, RMSE on training set: 0.9306289825476891.\n",
      "iter: 28, RMSE on training set: 0.9306306647233941.\n",
      "iter: 29, RMSE on training set: 0.9305439730863645.\n",
      "RMSE on test data: 0.9753618141289224 with gamma=0.04, k=4, lambda_u=0.1, lambda_i=0.05.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.1006294858410433.\n",
      "iter: 1, RMSE on training set: 1.071718684709675.\n",
      "iter: 2, RMSE on training set: 1.0425154253687194.\n",
      "iter: 3, RMSE on training set: 1.028598166116288.\n",
      "iter: 4, RMSE on training set: 1.0095959842342555.\n",
      "iter: 5, RMSE on training set: 0.9960580248540336.\n",
      "iter: 6, RMSE on training set: 0.9850400090123956.\n",
      "iter: 7, RMSE on training set: 0.9799138864273986.\n",
      "iter: 8, RMSE on training set: 0.9707590477047292.\n",
      "iter: 9, RMSE on training set: 0.9663928962361551.\n",
      "iter: 10, RMSE on training set: 0.9629627892162353.\n",
      "iter: 11, RMSE on training set: 0.9575166143247406.\n",
      "iter: 12, RMSE on training set: 0.9566104480585667.\n",
      "iter: 13, RMSE on training set: 0.9537236522362288.\n",
      "iter: 14, RMSE on training set: 0.9505731107593638.\n",
      "iter: 15, RMSE on training set: 0.9500119619490801.\n",
      "iter: 16, RMSE on training set: 0.9508285014709926.\n",
      "iter: 17, RMSE on training set: 0.9488851833094836.\n",
      "iter: 18, RMSE on training set: 0.9484816908951154.\n",
      "iter: 19, RMSE on training set: 0.948337372539384.\n",
      "iter: 20, RMSE on training set: 0.9479162636183533.\n",
      "iter: 21, RMSE on training set: 0.947118717489357.\n",
      "iter: 22, RMSE on training set: 0.9468693660086777.\n",
      "iter: 23, RMSE on training set: 0.9468960475853779.\n",
      "iter: 24, RMSE on training set: 0.9466128386588987.\n",
      "iter: 25, RMSE on training set: 0.946794293750251.\n",
      "iter: 26, RMSE on training set: 0.9465413186275824.\n",
      "iter: 27, RMSE on training set: 0.9463750622861494.\n",
      "iter: 28, RMSE on training set: 0.9463716257933324.\n",
      "iter: 29, RMSE on training set: 0.9463137624534573.\n",
      "RMSE on test data: 0.9832553595565572 with gamma=0.04, k=4, lambda_u=0.1, lambda_i=0.1.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0796065141890065.\n",
      "iter: 1, RMSE on training set: 1.0141138693173652.\n",
      "iter: 2, RMSE on training set: 0.9814611313387837.\n",
      "iter: 3, RMSE on training set: 0.9580017067820228.\n",
      "iter: 4, RMSE on training set: 0.9436686770772329.\n",
      "iter: 5, RMSE on training set: 0.9295865320461557.\n",
      "iter: 6, RMSE on training set: 0.9192258395254707.\n",
      "iter: 7, RMSE on training set: 0.9121802793200877.\n",
      "iter: 8, RMSE on training set: 0.905451706691753.\n",
      "iter: 9, RMSE on training set: 0.9004481944590269.\n",
      "iter: 10, RMSE on training set: 0.8968421328938039.\n",
      "iter: 11, RMSE on training set: 0.8942747429571578.\n",
      "iter: 12, RMSE on training set: 0.8908242787136728.\n",
      "iter: 13, RMSE on training set: 0.8889205368133714.\n",
      "iter: 14, RMSE on training set: 0.8878862540336958.\n",
      "iter: 15, RMSE on training set: 0.8864368730981815.\n",
      "iter: 16, RMSE on training set: 0.8859496027637618.\n",
      "iter: 17, RMSE on training set: 0.8850429851717888.\n",
      "iter: 18, RMSE on training set: 0.8846532980525713.\n",
      "iter: 19, RMSE on training set: 0.8841130109137486.\n",
      "iter: 20, RMSE on training set: 0.8838599916691955.\n",
      "iter: 21, RMSE on training set: 0.8836651204632666.\n",
      "iter: 22, RMSE on training set: 0.8834812996272174.\n",
      "iter: 23, RMSE on training set: 0.8833444590484636.\n",
      "iter: 24, RMSE on training set: 0.8832835928498604.\n",
      "iter: 25, RMSE on training set: 0.8831646837167472.\n",
      "iter: 26, RMSE on training set: 0.8831329619647398.\n",
      "iter: 27, RMSE on training set: 0.8830948185154031.\n",
      "iter: 28, RMSE on training set: 0.8830587558562515.\n",
      "iter: 29, RMSE on training set: 0.8830403440651987.\n",
      "RMSE on test data: 0.9762718807348795 with gamma=0.04, k=5, lambda_u=0.01, lambda_i=0.01.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0925019306985706.\n",
      "iter: 1, RMSE on training set: 1.0276156446235498.\n",
      "iter: 2, RMSE on training set: 0.9943404315016287.\n",
      "iter: 3, RMSE on training set: 0.9690580066918827.\n",
      "iter: 4, RMSE on training set: 0.9532373477057909.\n",
      "iter: 5, RMSE on training set: 0.9379853133346262.\n",
      "iter: 6, RMSE on training set: 0.9271435911478063.\n",
      "iter: 7, RMSE on training set: 0.9183157921020858.\n",
      "iter: 8, RMSE on training set: 0.9101756954941186.\n",
      "iter: 9, RMSE on training set: 0.9048567945280104.\n",
      "iter: 10, RMSE on training set: 0.9002452783685727.\n",
      "iter: 11, RMSE on training set: 0.8969411963587705.\n",
      "iter: 12, RMSE on training set: 0.8933413887206526.\n",
      "iter: 13, RMSE on training set: 0.8908148862311229.\n",
      "iter: 14, RMSE on training set: 0.8892258168902408.\n",
      "iter: 15, RMSE on training set: 0.8875692940175925.\n",
      "iter: 16, RMSE on training set: 0.8868560355726527.\n",
      "iter: 17, RMSE on training set: 0.8858109963956579.\n",
      "iter: 18, RMSE on training set: 0.8853503701885732.\n",
      "iter: 19, RMSE on training set: 0.884848197504565.\n",
      "iter: 20, RMSE on training set: 0.8846048241065307.\n",
      "iter: 21, RMSE on training set: 0.8843981364755784.\n",
      "iter: 22, RMSE on training set: 0.8842615407186293.\n",
      "iter: 23, RMSE on training set: 0.8841402011970462.\n",
      "iter: 24, RMSE on training set: 0.8841255868910942.\n",
      "iter: 25, RMSE on training set: 0.8840254123639647.\n",
      "iter: 26, RMSE on training set: 0.8840068230945675.\n",
      "iter: 27, RMSE on training set: 0.8839814146088291.\n",
      "iter: 28, RMSE on training set: 0.8839559918961085.\n",
      "iter: 29, RMSE on training set: 0.8839396028612659.\n",
      "RMSE on test data: 0.9731954478181709 with gamma=0.04, k=5, lambda_u=0.01, lambda_i=0.05.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.1114940976395509.\n",
      "iter: 1, RMSE on training set: 1.0498048352135398.\n",
      "iter: 2, RMSE on training set: 1.0160121668708166.\n",
      "iter: 3, RMSE on training set: 0.9887266312835912.\n",
      "iter: 4, RMSE on training set: 0.9709078740078609.\n",
      "iter: 5, RMSE on training set: 0.9538616419400675.\n",
      "iter: 6, RMSE on training set: 0.9416612499110549.\n",
      "iter: 7, RMSE on training set: 0.9306389645279259.\n",
      "iter: 8, RMSE on training set: 0.920528478051313.\n",
      "iter: 9, RMSE on training set: 0.9143490203258421.\n",
      "iter: 10, RMSE on training set: 0.9082954791890845.\n",
      "iter: 11, RMSE on training set: 0.9039120008020445.\n",
      "iter: 12, RMSE on training set: 0.8998107439002946.\n",
      "iter: 13, RMSE on training set: 0.896293956479058.\n",
      "iter: 14, RMSE on training set: 0.8940332910280142.\n",
      "iter: 15, RMSE on training set: 0.8918298006043671.\n",
      "iter: 16, RMSE on training set: 0.8906864027652277.\n",
      "iter: 17, RMSE on training set: 0.8892372071652053.\n",
      "iter: 18, RMSE on training set: 0.8884968748929405.\n",
      "iter: 19, RMSE on training set: 0.8878463236744251.\n",
      "iter: 20, RMSE on training set: 0.8874676685964056.\n",
      "iter: 21, RMSE on training set: 0.8871754326043458.\n",
      "iter: 22, RMSE on training set: 0.8869810913216972.\n",
      "iter: 23, RMSE on training set: 0.8868157743829657.\n",
      "iter: 24, RMSE on training set: 0.8867913256423708.\n",
      "iter: 25, RMSE on training set: 0.8866749748767253.\n",
      "iter: 26, RMSE on training set: 0.8866447636514136.\n",
      "iter: 27, RMSE on training set: 0.886611854926429.\n",
      "iter: 28, RMSE on training set: 0.886581413860101.\n",
      "iter: 29, RMSE on training set: 0.886562757546478.\n",
      "RMSE on test data: 0.9705613707650323 with gamma=0.04, k=5, lambda_u=0.01, lambda_i=0.1.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0755923286732247.\n",
      "iter: 1, RMSE on training set: 1.0230665678790225.\n",
      "iter: 2, RMSE on training set: 0.9953074437991131.\n",
      "iter: 3, RMSE on training set: 0.9746327379924422.\n",
      "iter: 4, RMSE on training set: 0.9626675971963284.\n",
      "iter: 5, RMSE on training set: 0.9461813161172296.\n",
      "iter: 6, RMSE on training set: 0.9349626119025867.\n",
      "iter: 7, RMSE on training set: 0.9282207161245711.\n",
      "iter: 8, RMSE on training set: 0.9209758457464492.\n",
      "iter: 9, RMSE on training set: 0.916375226352601.\n",
      "iter: 10, RMSE on training set: 0.9118292774039136.\n",
      "iter: 11, RMSE on training set: 0.9081910042410207.\n",
      "iter: 12, RMSE on training set: 0.9040343357775193.\n",
      "iter: 13, RMSE on training set: 0.9016842153671948.\n",
      "iter: 14, RMSE on training set: 0.900971902571742.\n",
      "iter: 15, RMSE on training set: 0.8983345561868668.\n",
      "iter: 16, RMSE on training set: 0.8980413503421605.\n",
      "iter: 17, RMSE on training set: 0.8962877244385035.\n",
      "iter: 18, RMSE on training set: 0.8956886501927556.\n",
      "iter: 19, RMSE on training set: 0.8947684164745324.\n",
      "iter: 20, RMSE on training set: 0.8943232890667281.\n",
      "iter: 21, RMSE on training set: 0.8939618582107669.\n",
      "iter: 22, RMSE on training set: 0.8935451854384807.\n",
      "iter: 23, RMSE on training set: 0.8932627218847816.\n",
      "iter: 24, RMSE on training set: 0.8931108383583708.\n",
      "iter: 25, RMSE on training set: 0.8928524310523055.\n",
      "iter: 26, RMSE on training set: 0.8927522969900423.\n",
      "iter: 27, RMSE on training set: 0.8926843801486724.\n",
      "iter: 28, RMSE on training set: 0.8925626459610976.\n",
      "iter: 29, RMSE on training set: 0.892515871766926.\n",
      "RMSE on test data: 0.9679573066511014 with gamma=0.04, k=5, lambda_u=0.05, lambda_i=0.01.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0814870047993017.\n",
      "iter: 1, RMSE on training set: 1.0269732308829957.\n",
      "iter: 2, RMSE on training set: 0.9983543003690176.\n",
      "iter: 3, RMSE on training set: 0.9757221304979672.\n",
      "iter: 4, RMSE on training set: 0.9623542712523225.\n",
      "iter: 5, RMSE on training set: 0.9472217859398651.\n",
      "iter: 6, RMSE on training set: 0.9366506441643253.\n",
      "iter: 7, RMSE on training set: 0.9294728058128995.\n",
      "iter: 8, RMSE on training set: 0.9222097252488108.\n",
      "iter: 9, RMSE on training set: 0.9180965453436049.\n",
      "iter: 10, RMSE on training set: 0.9135606684269831.\n",
      "iter: 11, RMSE on training set: 0.9107244869071232.\n",
      "iter: 12, RMSE on training set: 0.9072559141039402.\n",
      "iter: 13, RMSE on training set: 0.9052773773023864.\n",
      "iter: 14, RMSE on training set: 0.9044731758601541.\n",
      "iter: 15, RMSE on training set: 0.9025242690727299.\n",
      "iter: 16, RMSE on training set: 0.9023710326078895.\n",
      "iter: 17, RMSE on training set: 0.9008108200060797.\n",
      "iter: 18, RMSE on training set: 0.9004616102556986.\n",
      "iter: 19, RMSE on training set: 0.89973046667695.\n",
      "iter: 20, RMSE on training set: 0.899530323667431.\n",
      "iter: 21, RMSE on training set: 0.8991838243665009.\n",
      "iter: 22, RMSE on training set: 0.8989654475874932.\n",
      "iter: 23, RMSE on training set: 0.898748216386133.\n",
      "iter: 24, RMSE on training set: 0.898774614997096.\n",
      "iter: 25, RMSE on training set: 0.898510297257815.\n",
      "iter: 26, RMSE on training set: 0.8985052777959179.\n",
      "iter: 27, RMSE on training set: 0.8984699858794826.\n",
      "iter: 28, RMSE on training set: 0.8983985486417662.\n",
      "iter: 29, RMSE on training set: 0.8983557312006681.\n",
      "RMSE on test data: 0.966461772437097 with gamma=0.04, k=5, lambda_u=0.05, lambda_i=0.05.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0924630857907933.\n",
      "iter: 1, RMSE on training set: 1.039990303891617.\n",
      "iter: 2, RMSE on training set: 1.0121758587577392.\n",
      "iter: 3, RMSE on training set: 0.9885823449855261.\n",
      "iter: 4, RMSE on training set: 0.974708430668536.\n",
      "iter: 5, RMSE on training set: 0.9595102686533785.\n",
      "iter: 6, RMSE on training set: 0.9489976128441665.\n",
      "iter: 7, RMSE on training set: 0.9410188393192572.\n",
      "iter: 8, RMSE on training set: 0.9331358674439898.\n",
      "iter: 9, RMSE on training set: 0.9291572076827946.\n",
      "iter: 10, RMSE on training set: 0.92410116523499.\n",
      "iter: 11, RMSE on training set: 0.9211107791161676.\n",
      "iter: 12, RMSE on training set: 0.9177762016671959.\n",
      "iter: 13, RMSE on training set: 0.9156322278437127.\n",
      "iter: 14, RMSE on training set: 0.9145918391930459.\n",
      "iter: 15, RMSE on training set: 0.9127495998287518.\n",
      "iter: 16, RMSE on training set: 0.9126056314374631.\n",
      "iter: 17, RMSE on training set: 0.9109794976387668.\n",
      "iter: 18, RMSE on training set: 0.9106891326877049.\n",
      "iter: 19, RMSE on training set: 0.9100235856359763.\n",
      "iter: 20, RMSE on training set: 0.909890407162799.\n",
      "iter: 21, RMSE on training set: 0.9095591437327317.\n",
      "iter: 22, RMSE on training set: 0.9093930134453435.\n",
      "iter: 23, RMSE on training set: 0.9092001594457296.\n",
      "iter: 24, RMSE on training set: 0.9092854185611662.\n",
      "iter: 25, RMSE on training set: 0.9090171434879023.\n",
      "iter: 26, RMSE on training set: 0.9090402604610882.\n",
      "iter: 27, RMSE on training set: 0.9090130744416867.\n",
      "iter: 28, RMSE on training set: 0.9089592378011861.\n",
      "iter: 29, RMSE on training set: 0.9089193298796371.\n",
      "RMSE on test data: 0.9683278728084621 with gamma=0.04, k=5, lambda_u=0.05, lambda_i=0.1.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.091469762538895.\n",
      "iter: 1, RMSE on training set: 1.047440290162169.\n",
      "iter: 2, RMSE on training set: 1.0220635531928692.\n",
      "iter: 3, RMSE on training set: 1.0014879863784056.\n",
      "iter: 4, RMSE on training set: 0.9915425163827148.\n",
      "iter: 5, RMSE on training set: 0.969490343779442.\n",
      "iter: 6, RMSE on training set: 0.9566185339161102.\n",
      "iter: 7, RMSE on training set: 0.9492332013771627.\n",
      "iter: 8, RMSE on training set: 0.9403569487295079.\n",
      "iter: 9, RMSE on training set: 0.9357636159742174.\n",
      "iter: 10, RMSE on training set: 0.9300029820871644.\n",
      "iter: 11, RMSE on training set: 0.9245491063307899.\n",
      "iter: 12, RMSE on training set: 0.9194484193772726.\n",
      "iter: 13, RMSE on training set: 0.9162893873862236.\n",
      "iter: 14, RMSE on training set: 0.9157179477018288.\n",
      "iter: 15, RMSE on training set: 0.9118904294794198.\n",
      "iter: 16, RMSE on training set: 0.9116650791371355.\n",
      "iter: 17, RMSE on training set: 0.909022680128795.\n",
      "iter: 18, RMSE on training set: 0.9082233954844725.\n",
      "iter: 19, RMSE on training set: 0.9070346974493735.\n",
      "iter: 20, RMSE on training set: 0.9062864399015469.\n",
      "iter: 21, RMSE on training set: 0.9058111447977044.\n",
      "iter: 22, RMSE on training set: 0.9052121326725576.\n",
      "iter: 23, RMSE on training set: 0.9048038104867785.\n",
      "iter: 24, RMSE on training set: 0.9044845851828422.\n",
      "iter: 25, RMSE on training set: 0.9041914053229045.\n",
      "iter: 26, RMSE on training set: 0.9039816808448917.\n",
      "iter: 27, RMSE on training set: 0.9039112706265693.\n",
      "iter: 28, RMSE on training set: 0.9036932240314876.\n",
      "iter: 29, RMSE on training set: 0.9036265069702253.\n",
      "RMSE on test data: 0.9664334086790027 with gamma=0.04, k=5, lambda_u=0.1, lambda_i=0.01.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.093691285206742.\n",
      "iter: 1, RMSE on training set: 1.0470231519824076.\n",
      "iter: 2, RMSE on training set: 1.0219211921075586.\n",
      "iter: 3, RMSE on training set: 1.0002113490369282.\n",
      "iter: 4, RMSE on training set: 0.9882632367591341.\n",
      "iter: 5, RMSE on training set: 0.9706111138754642.\n",
      "iter: 6, RMSE on training set: 0.9591682305554879.\n",
      "iter: 7, RMSE on training set: 0.9519931259124771.\n",
      "iter: 8, RMSE on training set: 0.9439421621750439.\n",
      "iter: 9, RMSE on training set: 0.9404577990866066.\n",
      "iter: 10, RMSE on training set: 0.9350411302817369.\n",
      "iter: 11, RMSE on training set: 0.9314601339377173.\n",
      "iter: 12, RMSE on training set: 0.9276139155726632.\n",
      "iter: 13, RMSE on training set: 0.9252109472701802.\n",
      "iter: 14, RMSE on training set: 0.9248096520679465.\n",
      "iter: 15, RMSE on training set: 0.9220245965300168.\n",
      "iter: 16, RMSE on training set: 0.9221398677738357.\n",
      "iter: 17, RMSE on training set: 0.9197931035935856.\n",
      "iter: 18, RMSE on training set: 0.9193984923457251.\n",
      "iter: 19, RMSE on training set: 0.9184413934698039.\n",
      "iter: 20, RMSE on training set: 0.9181147029348267.\n",
      "iter: 21, RMSE on training set: 0.9176016288814584.\n",
      "iter: 22, RMSE on training set: 0.9173095695595702.\n",
      "iter: 23, RMSE on training set: 0.9169799375712437.\n",
      "iter: 24, RMSE on training set: 0.9169698174380829.\n",
      "iter: 25, RMSE on training set: 0.9165950794366465.\n",
      "iter: 26, RMSE on training set: 0.9165671333548897.\n",
      "iter: 27, RMSE on training set: 0.9165398411368261.\n",
      "iter: 28, RMSE on training set: 0.9163993864657182.\n",
      "iter: 29, RMSE on training set: 0.9163269871209648.\n",
      "RMSE on test data: 0.9693702099637611 with gamma=0.04, k=5, lambda_u=0.1, lambda_i=0.05.\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.101105634001546.\n",
      "iter: 1, RMSE on training set: 1.0567692194799523.\n",
      "iter: 2, RMSE on training set: 1.034605728706791.\n",
      "iter: 3, RMSE on training set: 1.0134623956720883.\n",
      "iter: 4, RMSE on training set: 1.0020637843474018.\n",
      "iter: 5, RMSE on training set: 0.9865211812011248.\n",
      "iter: 6, RMSE on training set: 0.9760905479991624.\n",
      "iter: 7, RMSE on training set: 0.969029227721333.\n",
      "iter: 8, RMSE on training set: 0.9611531117498671.\n",
      "iter: 9, RMSE on training set: 0.9583129188001801.\n",
      "iter: 10, RMSE on training set: 0.9527853638219576.\n",
      "iter: 11, RMSE on training set: 0.9497346115855376.\n",
      "iter: 12, RMSE on training set: 0.9464409207282924.\n",
      "iter: 13, RMSE on training set: 0.944206046904988.\n",
      "iter: 14, RMSE on training set: 0.9437949545245682.\n",
      "iter: 15, RMSE on training set: 0.941432995904806.\n",
      "iter: 16, RMSE on training set: 0.9417710250681957.\n",
      "iter: 17, RMSE on training set: 0.9394004268575501.\n",
      "iter: 18, RMSE on training set: 0.9392257160246323.\n",
      "iter: 19, RMSE on training set: 0.9383760232192074.\n",
      "iter: 20, RMSE on training set: 0.9382010032445389.\n",
      "iter: 21, RMSE on training set: 0.9377157265315279.\n",
      "iter: 22, RMSE on training set: 0.9375255909450853.\n",
      "iter: 23, RMSE on training set: 0.9372443017245191.\n",
      "iter: 24, RMSE on training set: 0.9373566561769971.\n",
      "iter: 25, RMSE on training set: 0.9369382482835058.\n",
      "iter: 26, RMSE on training set: 0.9369952368175235.\n",
      "iter: 27, RMSE on training set: 0.9369762905980138.\n",
      "iter: 28, RMSE on training set: 0.9368772278052543.\n",
      "iter: 29, RMSE on training set: 0.9368058847885505.\n",
      "RMSE on test data: 0.9788017487454269 with gamma=0.04, k=5, lambda_u=0.1, lambda_i=0.1.\n"
     ]
    }
   ],
   "source": [
    "for k in range(3, 6):\n",
    "    for lu in [0.01, 0.05, 0.1]:\n",
    "        for li in [0.01, 0.05, 0.1]:\n",
    "            W, Z = matrix_factorization_SGD(train, test, 0.04, k, lu, li)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0824886044642483.\n",
      "iter: 1, RMSE on training set: 1.061417538162265.\n",
      "iter: 2, RMSE on training set: 1.0310939135142811.\n",
      "iter: 3, RMSE on training set: 1.0111912533440692.\n",
      "iter: 4, RMSE on training set: 0.9895882251111173.\n",
      "iter: 5, RMSE on training set: 0.9761211274977003.\n",
      "iter: 6, RMSE on training set: 0.958255875864455.\n",
      "iter: 7, RMSE on training set: 0.95204508020081.\n",
      "iter: 8, RMSE on training set: 0.9430148003220992.\n",
      "iter: 9, RMSE on training set: 0.9347973447069295.\n",
      "iter: 10, RMSE on training set: 0.9332504281762514.\n",
      "iter: 11, RMSE on training set: 0.9254679177220466.\n",
      "iter: 12, RMSE on training set: 0.9209941740693552.\n",
      "iter: 13, RMSE on training set: 0.9195054094093281.\n",
      "iter: 14, RMSE on training set: 0.9167996443483255.\n",
      "iter: 15, RMSE on training set: 0.9145975415608443.\n",
      "iter: 16, RMSE on training set: 0.9131016524306037.\n",
      "iter: 17, RMSE on training set: 0.9110759433983577.\n",
      "iter: 18, RMSE on training set: 0.9109776653296944.\n",
      "iter: 19, RMSE on training set: 0.9098759488776484.\n",
      "iter: 20, RMSE on training set: 0.9090144739679986.\n",
      "iter: 21, RMSE on training set: 0.9085439439927555.\n",
      "iter: 22, RMSE on training set: 0.9078166349695457.\n",
      "iter: 23, RMSE on training set: 0.9073227014416183.\n",
      "iter: 24, RMSE on training set: 0.9072683668927718.\n",
      "iter: 25, RMSE on training set: 0.906714148280959.\n",
      "iter: 26, RMSE on training set: 0.9065850396116604.\n",
      "iter: 27, RMSE on training set: 0.9065125448948893.\n",
      "iter: 28, RMSE on training set: 0.9061077646551324.\n",
      "iter: 29, RMSE on training set: 0.9061993977869315.\n",
      "RMSE on test data: 1.1962670624403065 with gamma=0.04, k=5, lambda_u=0.1, lambda_i=0.01.\n"
     ]
    }
   ],
   "source": [
    "W, Z = matrix_factorization_SGD(ratings, test, 0.04, 5, 0.1, 0.01)\n",
    "prediction = W.dot(Z.T)\n",
    "create_csv_submission(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
